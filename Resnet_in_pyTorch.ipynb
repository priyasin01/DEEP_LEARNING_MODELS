{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet_in_pyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyasin01/DEEP_LEARNING_MODELS/blob/master/Resnet_in_pyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wAt7EJuwWK7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9a9926d2-3640-45b2-c5ec-f51ade000e08"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# The code below is derived from yunjey/pytorch-tutorial/tutorials/02-intermediate/deep_residual_network\n",
        "\n",
        "# ---------------------------------------------------------------------------- #\n",
        "# An implementation of https://arxiv.org/pdf/1512.03385.pdf                    #\n",
        "# See section 4.2 for the model architecture on CIFAR-10                       #\n",
        "# Some part of the code was referenced from below                              #\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py   #\n",
        "# ---------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5c702000 @  0x7faf7b2422a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LdKGh9RJWSGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1551
        },
        "outputId": "e31ceb68-8fc3-4771-c50a-596c1d3b810e"
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Hyper-parameterr\n",
        "batch_size = 200\n",
        "num_epochs = 80\n",
        "learning_rate = 0.03\n",
        "\n",
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform,\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
        "                                            train=False, \n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "shuffle=False)\n",
        "\n",
        "\n",
        "# 3x3 convolution\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                     stride=stride, padding=1, bias=False)\n",
        "\n",
        "# Residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = conv3x3(3, 16)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self.make_layer(block, 32, layers[0], 2)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[1], 2)\n",
        "        self.avg_pool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "        \n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if (stride != 1) or (self.in_channels != out_channels):\n",
        "            downsample = nn.Sequential(\n",
        "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "    \n",
        "model = ResNet(ResidualBlock, [2, 2, 2, 2]).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# For updating learning rate\n",
        "def update_lr(optimizer, lr):    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "before = time.time()\n",
        "log = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_total = 0\n",
        "    train_correct = 0\n",
        "    train_loss = 0\n",
        "    test_total = 0\n",
        "    test_correct = 0\n",
        "    test_loss = 0\n",
        "    \n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Accuracy calculation\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        train_loss += loss.item() * labels.size(0)\n",
        "    train_accuracy = train_correct / train_total\n",
        "\n",
        "    # Decay learning rate\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        curr_lr /= 3\n",
        "        update_lr(optimizer, curr_lr)\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    print(f'Epoch \\t {epoch+1} \\t {train_loss/train_total:.4f} \\t {train_correct/train_total:.4f} \\t {test_loss/test_total:.4f} \\t {test_correct/test_total:.4f}')\n",
        "    log.append([train_loss/train_total, train_correct/train_total, test_loss/test_total, test_correct/test_total])\n",
        "print(f'elapsed time: {time.time() - before}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "Epoch \t 1 \t 1.8917 \t 0.2912 \t 1.7312 \t 0.3430\n",
            "Epoch \t 2 \t 1.5978 \t 0.4082 \t 1.6549 \t 0.4008\n",
            "Epoch \t 3 \t 1.4059 \t 0.4864 \t 1.8376 \t 0.3655\n",
            "Epoch \t 4 \t 1.2778 \t 0.5367 \t 1.5059 \t 0.4806\n",
            "Epoch \t 5 \t 1.1783 \t 0.5751 \t 1.3339 \t 0.5258\n",
            "Epoch \t 6 \t 1.0918 \t 0.6081 \t 1.2114 \t 0.5676\n",
            "Epoch \t 7 \t 1.0268 \t 0.6317 \t 1.2839 \t 0.5462\n",
            "Epoch \t 8 \t 0.9704 \t 0.6499 \t 1.1437 \t 0.5891\n",
            "Epoch \t 9 \t 0.9258 \t 0.6680 \t 0.9807 \t 0.6541\n",
            "Epoch \t 10 \t 0.8880 \t 0.6831 \t 1.0670 \t 0.6230\n",
            "Epoch \t 11 \t 0.8474 \t 0.6988 \t 1.0158 \t 0.6458\n",
            "Epoch \t 12 \t 0.8145 \t 0.7102 \t 1.0909 \t 0.6296\n",
            "Epoch \t 13 \t 0.7908 \t 0.7204 \t 0.9497 \t 0.6703\n",
            "Epoch \t 14 \t 0.7603 \t 0.7331 \t 0.9821 \t 0.6667\n",
            "Epoch \t 15 \t 0.7400 \t 0.7391 \t 0.8199 \t 0.7183\n",
            "Epoch \t 16 \t 0.7115 \t 0.7503 \t 0.7862 \t 0.7220\n",
            "Epoch \t 17 \t 0.6921 \t 0.7569 \t 1.0372 \t 0.6661\n",
            "Epoch \t 18 \t 0.6670 \t 0.7656 \t 0.9409 \t 0.6771\n",
            "Epoch \t 19 \t 0.6490 \t 0.7727 \t 0.8596 \t 0.7110\n",
            "Epoch \t 20 \t 0.6313 \t 0.7771 \t 0.8805 \t 0.6959\n",
            "Epoch \t 21 \t 0.5696 \t 0.8009 \t 0.5922 \t 0.7938\n",
            "Epoch \t 22 \t 0.5596 \t 0.8045 \t 0.6037 \t 0.7912\n",
            "Epoch \t 23 \t 0.5535 \t 0.8064 \t 0.5880 \t 0.7959\n",
            "Epoch \t 24 \t 0.5436 \t 0.8118 \t 0.6300 \t 0.7841\n",
            "Epoch \t 25 \t 0.5399 \t 0.8128 \t 0.6681 \t 0.7725\n",
            "Epoch \t 26 \t 0.5355 \t 0.8141 \t 0.6285 \t 0.7846\n",
            "Epoch \t 27 \t 0.5319 \t 0.8155 \t 0.6027 \t 0.7918\n",
            "Epoch \t 28 \t 0.5262 \t 0.8172 \t 0.5773 \t 0.7978\n",
            "Epoch \t 29 \t 0.5207 \t 0.8198 \t 0.5960 \t 0.7949\n",
            "Epoch \t 30 \t 0.5168 \t 0.8207 \t 0.5753 \t 0.8001\n",
            "Epoch \t 31 \t 0.5132 \t 0.8221 \t 0.5914 \t 0.7988\n",
            "Epoch \t 32 \t 0.5072 \t 0.8235 \t 0.5969 \t 0.7934\n",
            "Epoch \t 33 \t 0.4997 \t 0.8251 \t 0.5683 \t 0.8067\n",
            "Epoch \t 34 \t 0.4993 \t 0.8267 \t 0.5608 \t 0.8103\n",
            "Epoch \t 35 \t 0.4950 \t 0.8281 \t 0.6058 \t 0.7934\n",
            "Epoch \t 36 \t 0.4895 \t 0.8296 \t 0.5850 \t 0.8007\n",
            "Epoch \t 37 \t 0.4924 \t 0.8289 \t 0.6261 \t 0.7885\n",
            "Epoch \t 38 \t 0.4841 \t 0.8324 \t 0.7143 \t 0.7540\n",
            "Epoch \t 39 \t 0.4758 \t 0.8355 \t 0.6810 \t 0.7749\n",
            "Epoch \t 40 \t 0.4751 \t 0.8338 \t 0.5908 \t 0.7990\n",
            "Epoch \t 41 \t 0.4518 \t 0.8434 \t 0.5227 \t 0.8229\n",
            "Epoch \t 42 \t 0.4498 \t 0.8440 \t 0.5205 \t 0.8256\n",
            "Epoch \t 43 \t 0.4460 \t 0.8454 \t 0.5235 \t 0.8234\n",
            "Epoch \t 44 \t 0.4436 \t 0.8473 \t 0.5066 \t 0.8287\n",
            "Epoch \t 45 \t 0.4453 \t 0.8448 \t 0.5189 \t 0.8250\n",
            "Epoch \t 46 \t 0.4413 \t 0.8481 \t 0.5281 \t 0.8213\n",
            "Epoch \t 47 \t 0.4364 \t 0.8489 \t 0.5224 \t 0.8241\n",
            "Epoch \t 48 \t 0.4351 \t 0.8483 \t 0.5407 \t 0.8157\n",
            "Epoch \t 49 \t 0.4355 \t 0.8482 \t 0.5307 \t 0.8183\n",
            "Epoch \t 50 \t 0.4388 \t 0.8479 \t 0.5135 \t 0.8273\n",
            "Epoch \t 51 \t 0.4349 \t 0.8479 \t 0.5237 \t 0.8206\n",
            "Epoch \t 52 \t 0.4339 \t 0.8502 \t 0.5151 \t 0.8249\n",
            "Epoch \t 53 \t 0.4311 \t 0.8503 \t 0.5087 \t 0.8293\n",
            "Epoch \t 54 \t 0.4301 \t 0.8502 \t 0.5040 \t 0.8303\n",
            "Epoch \t 55 \t 0.4286 \t 0.8517 \t 0.5319 \t 0.8197\n",
            "Epoch \t 56 \t 0.4279 \t 0.8510 \t 0.5080 \t 0.8291\n",
            "Epoch \t 57 \t 0.4262 \t 0.8512 \t 0.5100 \t 0.8301\n",
            "Epoch \t 58 \t 0.4244 \t 0.8533 \t 0.5144 \t 0.8244\n",
            "Epoch \t 59 \t 0.4261 \t 0.8513 \t 0.5058 \t 0.8299\n",
            "Epoch \t 60 \t 0.4222 \t 0.8539 \t 0.5174 \t 0.8248\n",
            "Epoch \t 61 \t 0.4156 \t 0.8557 \t 0.4887 \t 0.8359\n",
            "Epoch \t 62 \t 0.4128 \t 0.8576 \t 0.4924 \t 0.8348\n",
            "Epoch \t 63 \t 0.4137 \t 0.8570 \t 0.4905 \t 0.8368\n",
            "Epoch \t 64 \t 0.4127 \t 0.8565 \t 0.4897 \t 0.8350\n",
            "Epoch \t 65 \t 0.4087 \t 0.8581 \t 0.4915 \t 0.8343\n",
            "Epoch \t 66 \t 0.4116 \t 0.8577 \t 0.4891 \t 0.8385\n",
            "Epoch \t 67 \t 0.4093 \t 0.8589 \t 0.4948 \t 0.8353\n",
            "Epoch \t 68 \t 0.4089 \t 0.8591 \t 0.4908 \t 0.8341\n",
            "Epoch \t 69 \t 0.4084 \t 0.8585 \t 0.4909 \t 0.8359\n",
            "Epoch \t 70 \t 0.4064 \t 0.8599 \t 0.4923 \t 0.8348\n",
            "Epoch \t 71 \t 0.4098 \t 0.8580 \t 0.4901 \t 0.8358\n",
            "Epoch \t 72 \t 0.4070 \t 0.8596 \t 0.4914 \t 0.8380\n",
            "Epoch \t 73 \t 0.4058 \t 0.8603 \t 0.4981 \t 0.8331\n",
            "Epoch \t 74 \t 0.4044 \t 0.8607 \t 0.4874 \t 0.8383\n",
            "Epoch \t 75 \t 0.4090 \t 0.8592 \t 0.4916 \t 0.8350\n",
            "Epoch \t 76 \t 0.4032 \t 0.8604 \t 0.4957 \t 0.8345\n",
            "Epoch \t 77 \t 0.4051 \t 0.8603 \t 0.4890 \t 0.8363\n",
            "Epoch \t 78 \t 0.4012 \t 0.8604 \t 0.4891 \t 0.8375\n",
            "Epoch \t 79 \t 0.4041 \t 0.8604 \t 0.4891 \t 0.8381\n",
            "Epoch \t 80 \t 0.4028 \t 0.8604 \t 0.4925 \t 0.8346\n",
            "elapsed time: 2820.5536336898804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CY68OIm4XxtL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}